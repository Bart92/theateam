{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 0/315725 tweets with 0 exceptions\n",
      "Parsed 31572/315725 tweets with 33 exceptions\n",
      "Parsed 63144/315725 tweets with 33 exceptions\n",
      "Parsed 94716/315725 tweets with 33 exceptions\n",
      "Parsed 126288/315725 tweets with 33 exceptions\n",
      "Parsed 157860/315725 tweets with 33 exceptions\n",
      "Parsed 189432/315725 tweets with 33 exceptions\n",
      "Parsed 221004/315725 tweets with 34 exceptions\n",
      "Parsed 252576/315725 tweets with 34 exceptions\n",
      "Parsed 284148/315725 tweets with 34 exceptions\n",
      "Parsed 315720/315725 tweets with 34 exceptions\n",
      "Job finished. Go home and drink a beer.\n",
      "Load data...\n",
      "Nog even, en het internetbankieren bij ING ligt helemaal plat. #tragesite #backupvanhetjaaraanhetdraaien\n",
      "Step 1: Neutral loading done\n",
      "Step 2: Annotated loading done\n",
      "Padding length determined as 136\n",
      "Step 3: Padding done with 315691 annotated tweets and 89416 neutral tweets.\n",
      "3 done\n",
      "4 done\n",
      "[122030   3624      1      1      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0]\n",
      "[   298  16935 289252 289253   2884   9243     70   1665 289254   4802\n",
      "     32      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0]\n",
      "... data loading complete.\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Extract Transform and Load\n",
    "# Assumptions: tweets are stored in a tsv file\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import pandas as pd\n",
    "import ETL\n",
    "\n",
    "fpath = r\"/Users/Bart/Desktop/AITeam/semeval1.tsv\"\n",
    "\n",
    "#loads both annotated and neutral tweets\n",
    "percentageOfAnnotated = 0.2\n",
    "x_train, y_train, x_test, y_test, vocabulary_inv, neutral_tweets = ETL.main(fpath, percentageOfAnnotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "### Part 2A: Hyperparameters\n",
    "\"\"\"\n",
    "Train convolutional network for sentiment analysis on IMDB corpus. Based on\n",
    "\"Convolutional Neural Networks for Sentence Classification\" by Yoon Kim\n",
    "http://arxiv.org/pdf/1408.5882v2.pdf\n",
    "\n",
    "For \"CNN-rand\" and \"CNN-non-static\" gets to 88-90%, and \"CNN-static\" - 85% after 2-5 epochs with following settings:\n",
    "embedding_dim = 50          \n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "Differences from original article:\n",
    "- larger IMDB corpus, longer sentences; sentence length is very important, just like data size\n",
    "- smaller embedding dimension, 50 instead of 300\n",
    "- 2 filter sizes instead of original 3\n",
    "- fewer filters; original work uses 100, experiments show that 3-10 is enough;\n",
    "- random initialization is no worse than word2vec init on IMDB corpus\n",
    "- sliding Max Pooling instead of original Global Pooling\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "model_type = \"CNN-non-static\"  \n",
    "\n",
    "# Data source\n",
    "data_source = \"local_dir\"\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 20 #10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 1 #10\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 140 #400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "# ---------------------- Parameters end -----------------------\n",
    "print(\"Completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"tragesite\" in vocabulary_inv.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(284121, 136), (284121,)]\n",
      "[array([136527,      2,    126,      4,     54,    659,     23,    230,\n",
      "           93,      4,     53,    114,    185,      7,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0]), 1]\n"
     ]
    }
   ],
   "source": [
    "print([x_train.shape, y_train.shape])\n",
    "print([x_train[1],y_train[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting sequence length for actual size\n",
      "x_train shape: (284121, 136)\n",
      "x_test shape: (31570, 136)\n",
      "Vocabulary Size: 408513\n",
      "Model type is CNN-non-static\n",
      "Initiating word2vec.\n",
      "Load existing Word2Vec model '50features_1minwords_10context'\n",
      "Word2vec done.\n",
      "Initializing embedding layer with word2vec weights, shape (408513, 50)\n"
     ]
    }
   ],
   "source": [
    "### Part 2B: Network definition & word2vec training\n",
    "### make sure to delete existing word2vec model if you want to udate it\n",
    "from w2v import train_word2vec\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "if sequence_length != x_test.shape[1]:\n",
    "    print(\"Adjusting sequence length for actual size\")\n",
    "    sequence_length = x_test.shape[1]\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocabulary_inv)))\n",
    "\n",
    "# Prepare embedding layer weights and convert inputs for static model\n",
    "print(\"Model type is\", model_type)\n",
    "if model_type in [\"CNN-non-static\", \"CNN-static\"]:\n",
    "    print('Initiating word2vec.')\n",
    "    embedding_weights = train_word2vec(np.vstack((x_train, x_test, neutral_tweets)), vocabulary_inv, num_features=embedding_dim,\n",
    "                                       min_word_count=min_word_count, context=context)\n",
    "    print('Word2vec done.')\n",
    "    if model_type == \"CNN-static\":\n",
    "        x_train = np.stack([np.stack([embedding_weights[word] for word in sentence]) for sentence in x_train])\n",
    "        x_test = np.stack([np.stack([embedding_weights[word] for word in sentence]) for sentence in x_test])\n",
    "        print(\"x_train static shape:\", x_train.shape)\n",
    "        print(\"x_test static shape:\", x_test.shape)\n",
    "\n",
    "elif model_type == \"CNN-rand\":\n",
    "    embedding_weights = None\n",
    "else:\n",
    "    raise ValueError(\"Unknown model type\")\n",
    "\n",
    "# Build model\n",
    "if model_type == \"CNN-static\":\n",
    "    input_shape = (sequence_length, embedding_dim)\n",
    "else:\n",
    "    input_shape = (sequence_length,)\n",
    "\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "# Static model does not have embedding layer\n",
    "if model_type == \"CNN-static\":\n",
    "    z = model_input\n",
    "else:\n",
    "    z = Embedding(len(vocabulary_inv), embedding_dim, input_length=sequence_length, name=\"embedding\")(model_input)\n",
    "\n",
    "z = Dropout(dropout_prob[0])(z)\n",
    "\n",
    "# Convolutional block\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
    "model_output = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "if model_type == \"CNN-non-static\":\n",
    "    weights = np.array([v for v in embedding_weights.values()])\n",
    "    print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "    embedding_layer = model.get_layer(\"embedding\")\n",
    "    embedding_layer.set_weights([weights])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 284121 samples, validate on 31570 samples\n",
      "Epoch 1/1\n",
      "284121/284121 [==============================] - 2496s - loss: 0.5207 - acc: 0.7398 - val_loss: 0.4599 - val_acc: 0.7889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x112d85fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 3: Training the model\n",
    "# one epoch performs better on new data\n",
    "# on 16th of Oct, training with 350k annon and 130k neutral took about 260sec\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(x_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.save(\"/Users/Bart/Desktop/AITeam/AI_team_CNN_v0.2/models/theperfectmodel.h5\")\n",
    "from keras.models import load_model\n",
    "model = load_model(\"/Users/Bart/Desktop/AITeam/AI_team_CNN_v0.2/models/theperfectmodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 485: expected 2 fields, saw 6\\nSkipping line 7898: expected 2 fields, saw 6\\nSkipping line 8039: expected 2 fields, saw 6\\nSkipping line 16598: expected 2 fields, saw 6\\nSkipping line 17199: expected 2 fields, saw 6\\n'\n",
      "b'Skipping line 1889: expected 2 fields, saw 6\\nSkipping line 3489: expected 2 fields, saw 6\\nSkipping line 5346: expected 2 fields, saw 6\\nSkipping line 5560: expected 2 fields, saw 6\\nSkipping line 7894: expected 2 fields, saw 6\\nSkipping line 9838: expected 2 fields, saw 6\\nSkipping line 15582: expected 2 fields, saw 6\\nSkipping line 15637: expected 2 fields, saw 6\\nSkipping line 16868: expected 2 fields, saw 6\\nSkipping line 22888: expected 2 fields, saw 6\\nSkipping line 24481: expected 2 fields, saw 6\\nSkipping line 26265: expected 2 fields, saw 6\\nSkipping line 31100: expected 2 fields, saw 6\\nSkipping line 31788: expected 2 fields, saw 6\\nSkipping line 32804: expected 2 fields, saw 6\\nSkipping line 33690: expected 2 fields, saw 6\\nSkipping line 33907: expected 2 fields, saw 6\\nSkipping line 34363: expected 2 fields, saw 6\\nSkipping line 35584: expected 2 fields, saw 6\\nSkipping line 35906: expected 2 fields, saw 6\\nSkipping line 37042: expected 2 fields, saw 6\\nSkipping line 40361: expected 2 fields, saw 6\\nSkipping line 40500: expected 2 fields, saw 6\\nSkipping line 42879: expected 2 fields, saw 6\\nSkipping line 43599: expected 2 fields, saw 6\\nSkipping line 43661: expected 2 fields, saw 6\\nSkipping line 43787: expected 2 fields, saw 6\\nSkipping line 43985: expected 2 fields, saw 6\\nSkipping line 47347: expected 2 fields, saw 6\\n'\n"
     ]
    }
   ],
   "source": [
    "import assignSentiment\n",
    "\n",
    "fpath = r'/Users/Bart/Desktop/AITeam/TweetsSplitByCompany/'\n",
    "\n",
    "#This is very slow, 600 lines per minute for me, shows progress every 10k lines\n",
    "assignSentiment.test(fpath, vocabulary_inv, model, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import assignSentiment\n",
    "assignSentiment.test(fpath, vocabulary_inv, model, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Optional writing in a command line:\n",
    "import classify_sentiment\n",
    "file = open('save_examples.tsv', \"w\")\n",
    "command=[]\n",
    "while command!='quit':\n",
    "    command = input()\n",
    "    y = classify_sentiment.checkTweet(command,vocabulary_inv, model, sequence_length)\n",
    "    output = str(y)+' '+str(classify_sentiment.val2sen(y))+r' Tweet: \"'+str(command)+'\"'\n",
    "    print(output)\n",
    "    file.write(output+\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"/Users/Bart/Desktop/AITeam/AI_team_CNN_v0.2/models/amodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_helpers_neutrals import *\n",
    "\n",
    "print(load_neutral_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
